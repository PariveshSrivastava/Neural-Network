{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c0613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1ea3c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    # builds a set of all nodes and edges in a graph\n",
    "    nodes, edges = set(), set()\n",
    "\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format=\"svg\", graph_attr={\"rankdir\": \"LR\"})  # LR = left to right\n",
    "\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        # for any value in the graph, create a rectangular ('record') node for it\n",
    "        dot.node(name=uid, label=\"{ %s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad,), shape=\"record\")\n",
    "\n",
    "        if n._op:\n",
    "            # if this value is a result of some operation, create an op node for it\n",
    "            dot.node(name=uid + n._op, label=n._op)\n",
    "            # and connect this node to it\n",
    "            dot.edge(uid + n._op, uid)\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        # connect n1 to the op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "aef246ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self ,data ,_children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance( other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data , (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float))\n",
    "        out = Value(self.data**other, (self,), 'pow')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out \n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        \n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4f101a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(np.random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(np.random.uniform(-1,1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = sum((wi*xi for wi,xi in zip(self.w,x)), self.b)\n",
    "        return act.tanh() \n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ebd43444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=-0.5992375648994782)\n"
     ]
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "mlp = MLP(3, [4, 4, 1])\n",
    "print(mlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "aa38c31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(data=0.2856164769992924), Value(data=0.829491087038833), Value(data=-0.8241300433776473), Value(data=-0.021863898960120975), Value(data=0.8456491230272654), Value(data=0.8905489381366243), Value(data=-0.634936340321693), Value(data=0.1717848037249019), Value(data=-0.11893291180242715), Value(data=0.1946579279152687), Value(data=0.4806020770799728), Value(data=-0.8842687307698123), Value(data=0.0476534519720595), Value(data=-0.7964900188865289), Value(data=0.2762122376078642), Value(data=0.11684188562997488), Value(data=0.03348605033825924), Value(data=0.6015311566384114), Value(data=-0.7689541451647546), Value(data=0.7677798719612567), Value(data=-0.02459519355915263), Value(data=-0.24141696017834247), Value(data=0.4990749466692386), Value(data=-0.850944892426384), Value(data=-0.03435385503020982), Value(data=0.6717540203662693), Value(data=0.3273581225158382), Value(data=0.3673780245391556), Value(data=0.8634358501246815), Value(data=-0.38470631122072896), Value(data=-0.10969733330724263), Value(data=-0.642657659278302), Value(data=0.35103612177628984), Value(data=0.5603667268183008), Value(data=0.9334313081757657), Value(data=0.589474288822206), Value(data=-0.3709002191449824), Value(data=-0.8145980238967374), Value(data=-0.2899825249395913), Value(data=0.4009615572680343), Value(data=0.6143073412553615)]\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(mlp.parameters())\n",
    "print(len(mlp.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "44488ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.5992375648994782),\n",
       " Value(data=0.058731446879178514),\n",
       " Value(data=-0.3294919561196667),\n",
       " Value(data=-0.5887996559988128)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 2.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "ypred = [mlp(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "67e44fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=6.6523384494066695)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = [(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]\n",
    "loss = sum(loss,0)\n",
    "loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a7a50d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8fbf13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in mlp.parameters():\n",
    "    p.data += -0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8f2d6f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=6.317441609234911)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new loss\n",
    "ypred = [mlp(x) for x in xs]\n",
    "loss = [(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]\n",
    "loss = sum(loss)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3ee05223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(val):\n",
    "    # forward pass\n",
    "    ypred = [mlp(x) for x in xs]\n",
    "    loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "\n",
    "    # zero gradients before backward pass\n",
    "    for p in mlp.parameters():\n",
    "        p.grad = 0.0\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward() # gradient descent algo\n",
    "\n",
    "    # update weights using gradients\n",
    "    for p in mlp.parameters():\n",
    "        p.data += -0.05 * p.grad\n",
    "\n",
    "    print(\"iter \", val, \"loss \", loss.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8864ae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  0 loss  6.317441609234911\n",
      "iter  1 loss  4.696570665979875\n",
      "iter  2 loss  3.6556744194484994\n",
      "iter  3 loss  3.177821173125466\n",
      "iter  4 loss  2.881715967650859\n",
      "iter  5 loss  2.6347939261569557\n",
      "iter  6 loss  2.343212629794987\n",
      "iter  7 loss  1.9000368304972688\n",
      "iter  8 loss  1.2762820391090306\n",
      "iter  9 loss  0.7485536297320639\n",
      "iter  10 loss  0.4706883554430532\n",
      "iter  11 loss  0.32915375022954496\n",
      "iter  12 loss  0.2483464233471812\n",
      "iter  13 loss  0.19734644260675954\n",
      "iter  14 loss  0.16267830566647562\n",
      "iter  15 loss  0.13777587629697405\n",
      "iter  16 loss  0.119120177581792\n",
      "iter  17 loss  0.10467649284900188\n",
      "iter  18 loss  0.09319478523423372\n",
      "iter  19 loss  0.08386844160433204\n",
      "iter  20 loss  0.0761555157620263\n",
      "iter  21 loss  0.06967938302124846\n",
      "iter  22 loss  0.06417071748610534\n",
      "iter  23 loss  0.05943214557304995\n",
      "iter  24 loss  0.05531592110088682\n",
      "iter  25 loss  0.05170937633939627\n",
      "iter  26 loss  0.04852517746190569\n",
      "iter  27 loss  0.04569463851113774\n",
      "iter  28 loss  0.04316303460507102\n",
      "iter  29 loss  0.04088625310846568\n",
      "iter  30 loss  0.038828359310191796\n",
      "iter  31 loss  0.03695979916313672\n",
      "iter  32 loss  0.03525605351829399\n",
      "iter  33 loss  0.03369661738767966\n",
      "iter  34 loss  0.032264216565054155\n",
      "iter  35 loss  0.030944199869920176\n",
      "iter  36 loss  0.029724062915159215\n",
      "iter  37 loss  0.028593071476415093\n",
      "iter  38 loss  0.027541961071848087\n",
      "iter  39 loss  0.026562695416190454\n",
      "iter  40 loss  0.025648270764535592\n",
      "iter  41 loss  0.024792556324401402\n",
      "iter  42 loss  0.023990163238572282\n",
      "iter  43 loss  0.023236336365814556\n",
      "iter  44 loss  0.02252686437840098\n",
      "iter  45 loss  0.021858004671621897\n",
      "iter  46 loss  0.0212264203243304\n",
      "iter  47 loss  0.02062912692082669\n",
      "iter  48 loss  0.020063447486330523\n",
      "iter  49 loss  0.01952697413258111\n",
      "iter  50 loss  0.019017535280105614\n",
      "iter  51 loss  0.018533167536758096\n",
      "iter  52 loss  0.018072091481275723\n",
      "iter  53 loss  0.017632690735632176\n",
      "iter  54 loss  0.017213493818356507\n",
      "iter  55 loss  0.016813158358434853\n",
      "iter  56 loss  0.0164304573203118\n",
      "iter  57 loss  0.016064266948264076\n",
      "iter  58 loss  0.015713556185675333\n",
      "iter  59 loss  0.015377377363575112\n",
      "iter  60 loss  0.015054857984848331\n",
      "iter  61 loss  0.01474519345706802\n",
      "iter  62 loss  0.014447640648976454\n",
      "iter  63 loss  0.014161512164062175\n",
      "iter  64 loss  0.013886171240110722\n",
      "iter  65 loss  0.013621027196569113\n",
      "iter  66 loss  0.013365531362497421\n",
      "iter  67 loss  0.013119173427121955\n",
      "iter  68 loss  0.012881478162846662\n",
      "iter  69 loss  0.012652002477249371\n",
      "iter  70 loss  0.012430332756278839\n",
      "iter  71 loss  0.012216082465737684\n",
      "iter  72 loss  0.012008889982308871\n",
      "iter  73 loss  0.01180841662897474\n",
      "iter  74 loss  0.011614344892770063\n",
      "iter  75 loss  0.01142637680548399\n",
      "iter  76 loss  0.01124423247024061\n",
      "iter  77 loss  0.01106764871889681\n",
      "iter  78 loss  0.010896377886943115\n",
      "iter  79 loss  0.010730186694116293\n",
      "iter  80 loss  0.010568855220262852\n",
      "iter  81 loss  0.01041217596715578\n",
      "iter  82 loss  0.010259952997988734\n",
      "iter  83 loss  0.010112001147166594\n",
      "iter  84 loss  0.009968145293801764\n",
      "iter  85 loss  0.009828219693019873\n",
      "iter  86 loss  0.009692067359793659\n",
      "iter  87 loss  0.009559539500565484\n",
      "iter  88 loss  0.00943049498840242\n",
      "iter  89 loss  0.009304799877852602\n",
      "iter  90 loss  0.009182326956052712\n",
      "iter  91 loss  0.009062955326973501\n",
      "iter  92 loss  0.008946570025991621\n",
      "iter  93 loss  0.008833061662245168\n",
      "iter  94 loss  0.008722326086469475\n",
      "iter  95 loss  0.008614264082226722\n",
      "iter  96 loss  0.008508781078633617\n",
      "iter  97 loss  0.00840578688286604\n",
      "iter  98 loss  0.008305195430874001\n",
      "iter  99 loss  0.008206924554880939\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    train(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a9ddc75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9640100399465161),\n",
       " Value(data=-0.958690761496502),\n",
       " Value(data=-0.9493516264179659),\n",
       " Value(data=0.9495628350373376)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = [mlp(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd0074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
